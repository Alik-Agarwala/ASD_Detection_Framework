{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463957ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# A Unique Framework for Autism Spectrum Disorder Detection\n",
    "# Based on Improved BiSectional kMedoids Clustering and Ensemble Learning\n",
    "#\n",
    "# This notebook reproduces the experiments described in the research paper.\n",
    "# ==============================================================================\n",
    "\n",
    "# Section 4: Experimental Implementation - Library Imports\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.io import arff\n",
    "from warnings import filterwarnings\n",
    "\n",
    "# Pre-processing\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Clustering Algorithms from scikit-learn and scikit-learn-extra\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.cluster import BisectingKMeans # CORRECTED IMPORT\n",
    "\n",
    "# Classification Models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "import tensorflow as tf\n",
    "\n",
    "# Ensemble Learning\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Evaluation Metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, mean_squared_error\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully.\")\n",
    "print(\"TensorFlow Version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796565bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Section 3 & 4: Data Loading and Pre-processing\n",
    "# ==============================================================================\n",
    "def load_and_preprocess_data(arff_file_path):\n",
    "    \"\"\"\n",
    "    Loads data from an ARFF file and applies the pre-processing steps\n",
    "    described in the paper: missing value imputation, label encoding, and scaling.\n",
    "    \"\"\"\n",
    "    data, meta = arff.loadarff(arff_file_path)\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "    for col in df.select_dtypes(['object']).columns:\n",
    "        df[col] = df[col].str.decode('utf-8')\n",
    "\n",
    "    df.replace('?', np.nan, inplace=True)\n",
    "\n",
    "    num_imputer = SimpleImputer(strategy='mean')\n",
    "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'float64' or df[col].dtype == 'int64':\n",
    "            df[col] = num_imputer.fit_transform(df[[col]])\n",
    "        else:\n",
    "            imputed_data = cat_imputer.fit_transform(df[[col]])\n",
    "            df[col] = pd.Series(imputed_data.flatten())\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "\n",
    "    X = df.drop('Class/ASD', axis=1)\n",
    "    y = df['Class/ASD']\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "    return X_scaled, y\n",
    "\n",
    "print(\"Data pre-processing function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315b8af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically find the ARFF file in the current directory\n",
    "try:\n",
    "    arff_file = [f for f in os.listdir('.') if f.endswith('.arff')][0]\n",
    "    print(f\"Loading and processing dataset: {arff_file}\")\n",
    "    X, y = load_and_preprocess_data(arff_file)\n",
    "\n",
    "    # Split data for training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    print(f\"\\nData shapes:\\nX_train: {X_train.shape}\\ny_train: {y_train.shape}\\nX_test: {X_test.shape}\\ny_test: {y_test.shape}\")\n",
    "    print(\"\\nSample of pre-processed data:\")\n",
    "    display(X.head())\n",
    "except IndexError:\n",
    "    print(\"ERROR: Could not find the .arff dataset file in this directory.\")\n",
    "    print(\"Please make sure the dataset file (e.g., 'Autism-Child-Data.arff') is in the same folder as this notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9ff97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Section 4: Xie-Beni Index Implementation\n",
    "# ==============================================================================\n",
    "def xie_beni_index(X, labels, centers):\n",
    "    \"\"\"Calculates the Xie-Beni index for a given clustering result.\"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    n_clusters = len(centers)\n",
    "    \n",
    "    compactness = np.sum([np.sum((X[labels == i] - centers[i])**2) for i in range(n_clusters)])\n",
    "    compactness /= n_samples\n",
    "\n",
    "    min_separation = np.inf\n",
    "    if n_clusters > 1:\n",
    "        for i in range(n_clusters):\n",
    "            for j in range(i + 1, n_clusters):\n",
    "                dist = np.linalg.norm(centers[i] - centers[j])**2\n",
    "                if dist < min_separation:\n",
    "                    min_separation = dist\n",
    "    else:\n",
    "        return np.inf\n",
    "\n",
    "    if min_separation == 0:\n",
    "        return np.inf\n",
    "\n",
    "    return compactness / min_separation\n",
    "\n",
    "def find_optimal_k(X, clusterer_class, max_k=10):\n",
    "    \"\"\"Finds the optimal 'k' by minimizing the Xie-Beni index.\"\"\"\n",
    "    best_k = 2\n",
    "    min_xb_index = np.inf\n",
    "    print(f\"\\nFinding optimal 'k' for {clusterer_class.__name__} using Xie-Beni Index...\")\n",
    "    for k in range(2, max_k + 1):\n",
    "        # Handle different clusterer APIs\n",
    "        if 'KMedoids' in clusterer_class.__name__:\n",
    "             clusterer = clusterer_class(n_clusters=k, random_state=42, method='pam')\n",
    "             labels = clusterer.fit_predict(X)\n",
    "             centers = clusterer.cluster_centers_\n",
    "        else: # For KMeans and BisectingKMeans\n",
    "            clusterer = clusterer_class(n_clusters=k, random_state=42, n_init=10)\n",
    "            labels = clusterer.fit_predict(X)\n",
    "            centers = clusterer.cluster_centers_\n",
    "\n",
    "        xb_index = xie_beni_index(X.to_numpy(), labels, centers)\n",
    "        print(f\"  k={k}, Xie-Beni Index = {xb_index:.4f}\")\n",
    "        if xb_index < min_xb_index:\n",
    "            min_xb_index = xb_index\n",
    "            best_k = k\n",
    "    \n",
    "    print(f\"Optimal 'k' found: {best_k} with Xie-Beni Index: {min_xb_index:.4f}\")\n",
    "    return best_k\n",
    "\n",
    "print(\"Xie-Beni Index helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22e6f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Section 4: Defining Classification Models\n",
    "# ==============================================================================\n",
    "def create_ann_model(input_dim):\n",
    "    \"\"\"Helper function to create the TensorFlow ANN model as described in the paper.\"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(input_dim,)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.02)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize all models for the experiment\n",
    "models = {\n",
    "    'DT': DecisionTreeClassifier(random_state=42),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Naïve Bayes': GaussianNB(),\n",
    "    'SVM': SVC(kernel='rbf', gamma=0.15, probability=True, random_state=42),\n",
    "    'ANN': None # ANN is handled separately in the pipeline\n",
    "}\n",
    "\n",
    "print(\"Classifier models defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225c79ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Main Experiment Pipeline\n",
    "# ==============================================================================\n",
    "from sklearn.base import clone # Import the clone function\n",
    "\n",
    "def run_experiments(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Executes the full experimental pipeline as described in the paper.\n",
    "    This includes:\n",
    "    1. Evaluating baseline classifiers on un-clustered data.\n",
    "    2. Cascading clustering with classification to enhance performance.\n",
    "    3. Applying the proposed ensemble learning approach for final prediction.\n",
    "    \"\"\"\n",
    "    results_accuracy = pd.DataFrame(index=models.keys())\n",
    "    results_precision = pd.DataFrame(index=models.keys())\n",
    "    \n",
    "    # --- Stage 1: Classification Without Clustering ---\n",
    "    print(\"\\n--- Running Stage 1: Baseline Classifier Performance ---\")\n",
    "    acc_scores, prec_scores = {}, {}\n",
    "    trained_models_no_cluster = {}\n",
    "    for name, model_blueprint in models.items():\n",
    "        # Handle the custom TensorFlow ANN separately from scikit-learn models\n",
    "        if name == 'ANN':\n",
    "            ann_model = create_ann_model(X_train.shape[1])\n",
    "            ann_model.fit(X_train, y_train, epochs=80, batch_size=32, verbose=0)\n",
    "            y_pred = (ann_model.predict(X_test) > 0.5).astype(int)\n",
    "            trained_models_no_cluster[name] = ann_model\n",
    "        else:\n",
    "            # Use a fresh clone of the model for this stage to ensure independence\n",
    "            model = clone(model_blueprint)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            trained_models_no_cluster[name] = model\n",
    "        \n",
    "        acc_scores[name] = accuracy_score(y_test, y_pred) * 100\n",
    "        prec_scores[name] = precision_score(y_test, y_pred, zero_division=0) * 100\n",
    "    results_accuracy['Accuracy without Clustering (%)'] = acc_scores.values()\n",
    "    results_precision['Precision without Clustering (%)'] = prec_scores.values()\n",
    "    \n",
    "    # --- Stage 2: Cascading Clustering with Classification ---\n",
    "    # The dictionary key 'Bisectional kMedoids' is kept for table consistency with the paper's terminology\n",
    "    clustering_methods = {'kMeans': KMeans, 'kMedoids': KMedoids, 'Bisectional kMedoids': BisectingKMeans}\n",
    "    trained_models_clustered = {} # Stores models trained on the final clustered data\n",
    "    \n",
    "    for cluster_name, cluster_class in clustering_methods.items():\n",
    "        print(f\"\\n--- Running Stage 2: {cluster_name} + Classifiers ---\")\n",
    "        acc_scores, prec_scores = {}, {}\n",
    "        k = find_optimal_k(X_train, cluster_class)\n",
    "        cluster_model = cluster_class(n_clusters=k, random_state=42, n_init=10) if cluster_name != 'kMedoids' else cluster_class(n_clusters=k, random_state=42, method='pam')\n",
    "        \n",
    "        # Add cluster labels as a new feature\n",
    "        X_train_c = X_train.copy()\n",
    "        X_train_c['cluster'] = cluster_model.fit_predict(X_train)\n",
    "        X_test_c = X_test.copy()\n",
    "        X_test_c['cluster'] = cluster_model.predict(X_test)\n",
    "\n",
    "        for name, model_blueprint in models.items():\n",
    "            if name == 'ANN':\n",
    "                ann_model = create_ann_model(X_train_c.shape[1])\n",
    "                ann_model.fit(X_train_c, y_train, epochs=80, batch_size=32, verbose=0)\n",
    "                y_pred = (ann_model.predict(X_test_c) > 0.5).astype(int)\n",
    "                # Store the model trained on the paper's proposed clustering method\n",
    "                if cluster_name == 'Bisectional kMedoids':\n",
    "                    trained_models_clustered[name] = ann_model\n",
    "            else:\n",
    "                model = clone(model_blueprint)\n",
    "                model.fit(X_train_c, y_train)\n",
    "                y_pred = model.predict(X_test_c)\n",
    "                # Store the model trained on the paper's proposed clustering method\n",
    "                if cluster_name == 'Bisectional kMedoids':\n",
    "                    trained_models_clustered[name] = model\n",
    "\n",
    "            acc_scores[name] = accuracy_score(y_test, y_pred) * 100\n",
    "            prec_scores[name] = precision_score(y_test, y_pred, zero_division=0) * 100\n",
    "        \n",
    "        col_name = f'Accuracy with {cluster_name} Clustering (%)' if cluster_name != 'Bisectional kMedoids' else 'Accuracy with Xie Benie Bisectional kMedoids (%)'\n",
    "        prec_col_name = f'Precision with {cluster_name} Clustering (%)' if cluster_name != 'Bisectional kMedoids' else 'Precision with Xie Benie Bisectional kMedoids (%)'\n",
    "        results_accuracy[col_name] = acc_scores.values()\n",
    "        results_precision[prec_col_name] = prec_scores.values()\n",
    "        \n",
    "    # --- Stage 3: Proposed Ensemble Model and Final RMSE Calculation ---\n",
    "    print(\"\\n--- Running Stage 3: Proposed Ensemble Model and RMSE Calculation ---\")\n",
    "    \n",
    "    # Calculate baseline RMSE using a standard model (SVC) for a consistent baseline\n",
    "    y_pred_no_cluster = trained_models_no_cluster['SVM'].predict(X_test)\n",
    "    rmse_without_clustering = np.sqrt(mean_squared_error(y_test, y_pred_no_cluster))\n",
    "    \n",
    "    # Perform a manual Max Voting / Hard Voting ensemble as described in the paper's methodology\n",
    "    # This approach uses all five classifiers shown in the framework diagram\n",
    "    k_optimal = find_optimal_k(X_train, BisectingKMeans)\n",
    "    bkm = BisectingKMeans(n_clusters=k_optimal, random_state=42, n_init=10)\n",
    "    X_train_final = X_train.copy()\n",
    "    X_train_final['cluster'] = bkm.fit_predict(X_train)\n",
    "    X_test_final = X_test.copy()\n",
    "    X_test_final['cluster'] = bkm.predict(X_test)\n",
    "\n",
    "    # Get predictions from all 5 models trained on the final clustered data\n",
    "    preds_dt = trained_models_clustered['DT'].predict(X_test_final)\n",
    "    preds_knn = trained_models_clustered['KNN'].predict(X_test_final)\n",
    "    preds_nb = trained_models_clustered['Naïve Bayes'].predict(X_test_final)\n",
    "    preds_svm = trained_models_clustered['SVM'].predict(X_test_final)\n",
    "    preds_ann = (trained_models_clustered['ANN'].predict(X_test_final) > 0.5).astype(int).flatten()\n",
    "\n",
    "    # Stack predictions and perform a manual hard vote (find the most frequent prediction for each sample)\n",
    "    stacked_preds = np.vstack([preds_dt, preds_knn, preds_nb, preds_svm, preds_ann]).T\n",
    "    y_pred_ensemble = np.asarray([np.bincount(row).argmax() for row in stacked_preds])\n",
    "\n",
    "    # Calculate final metrics for the proposed model\n",
    "    ensemble_acc = accuracy_score(y_test, y_pred_ensemble) * 100\n",
    "    ensemble_prec = precision_score(y_test, y_pred_ensemble) * 100\n",
    "    rmse_proposed_model = np.sqrt(mean_squared_error(y_test, y_pred_ensemble))\n",
    "    \n",
    "    # Populate the results tables\n",
    "    acc_col_ensemble = 'Accuracy with Ensemble Learning(Proposed Model)'\n",
    "    prec_col_ensemble = 'Precision with Ensemble Learning(Proposed Model)'\n",
    "    results_accuracy[acc_col_ensemble] = [np.nan] * (len(models)-1) + [ensemble_acc]\n",
    "    results_precision[prec_col_ensemble] = [np.nan] * (len(models)-1) + [ensemble_prec]\n",
    "    \n",
    "    rmse_results = pd.DataFrame({\n",
    "        'RMSE without Clustering': [rmse_without_clustering],\n",
    "        'RMSE with Ensemble Learning(Proposed Model)': [rmse_proposed_model]\n",
    "    }, index=[arff_file.split('-')[1].capitalize()])\n",
    "\n",
    "    return results_accuracy, results_precision, rmse_results\n",
    "\n",
    "print(\"Main experiment pipeline function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721e56ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Section 5: Results & Analysis\n",
    "# ==============================================================================\n",
    "try:\n",
    "    final_accuracy, final_precision, final_rmse = run_experiments(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Reorder columns to match paper's table structure\n",
    "    acc_col_order = [\n",
    "        'Accuracy without Clustering (%)', 'Accuracy with kMeans Clustering (%)',\n",
    "        'Accuracy with kMedoids Clustering (%)', 'Accuracy with Xie Benie Bisectional kMedoids (%)',\n",
    "        'Accuracy with Ensemble Learning(Proposed Model)'\n",
    "    ]\n",
    "    prec_col_order = [\n",
    "        'Precision without Clustering (%)', 'Precision with kMeans Clustering (%)',\n",
    "        'Precision with kMedoids Clustering (%)', 'Precision with Xie Benie Bisectional kMedoids (%)',\n",
    "        'Precision with Ensemble Learning(Proposed Model)'\n",
    "    ]\n",
    "    final_accuracy = final_accuracy.reindex(columns=acc_col_order)\n",
    "    final_precision = final_precision.reindex(columns=prec_col_order)\n",
    "\n",
    "    print(\"\\n\\n\" + \"=\"*60)\n",
    "    print(f\"      FINAL RESULTS FOR: {arff_file.split('.')[0].upper()} \")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    print(f\"\\n--- Accuracy Performance (Similar to Tables 2, 4, 6) ---\")\n",
    "    display(final_accuracy.style.format(\"{:.1f}\").background_gradient(cmap='Greens', axis=1))\n",
    "\n",
    "    print(f\"\\n--- Precision Performance (Similar to Tables 3, 5, 7) ---\")\n",
    "    display(final_precision.style.format(\"{:.1f}\").background_gradient(cmap='Blues', axis=1))\n",
    "\n",
    "    print(f\"\\n--- RMSE Value Comparison (Similar to Table 8) ---\")\n",
    "    display(final_rmse.style.format(\"{:.3f}\").background_gradient(cmap='Reds', axis=1, subset=['RMSE with Ensemble Learning(Proposed Model)']))\n",
    "\n",
    "except NameError:\n",
    "     print(\"ERROR: Could not run experiments because the data was not loaded.\")\n",
    "     print(\"Please make sure the previous cells, especially the data loading cell, have run successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
